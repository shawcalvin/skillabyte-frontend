"use client"

import { ModuleContainer } from "@/components/interactive/modules";
import { ReviewQuiz } from "@/components/interactive/questions";
import { Text } from "@/components/ui/text";
import { LearningModulePageProps } from "@/lib/types/modules";

export default function ModulePage(props: LearningModulePageProps) {

    return (
        <>
            <ModuleContainer title={"Review Quiz"} isComplete={false} {...props}>
                <Text>Before continuing, please answer the following review questions. These questions are not scored, and you can retry them as many times as needed.</Text>
                <ReviewQuiz quiz={quiz} setIsComplete={() => props.setIsComplete(true)} />
            </ModuleContainer>
        </>
    );
}

const quiz = [
    {
        question: "What distinguishes Generative AI (GenAI) from traditional AI systems?",
        answers: [
            {
                answer: "GenAI analyzes existing data for insights", isCorrect: false, feedback: "Analyzing existing data is a characteristic of traditional AI, such as predictive analytics. GenAI goes beyond by creating new content."
            },
            {
                answer: "GenAI generates new content, such as text or images", isCorrect: true, feedback: "GenAI’s distinctive feature is its ability to generate new content, such as text, images, videos, or music, based on patterns learned from vast training data."
            },
            {
                answer: "GenAI uses exclusively numerical inputs to predict outcomes", isCorrect: false, feedback: "GenAI uses diverse forms of input, including text and images, not just numerical data."
            },
            {
                answer: "GenAI operates only on structured databases", isCorrect: false, feedback: "GenAI can operate on unstructured data like text or images, not just structured databases."
            },
        ]
    },
    {
        question: "What are hallucinations in the context of Generative AI?",
        answers: [
            {
                answer: "Errors caused by faulty programming", isCorrect: false, feedback: "Hallucinations are not necessarily due to faulty programming but are a limitation inherent to generative processes."
            },
            {
                answer: "Unexpected but accurate outputs", isCorrect: false, feedback: "Hallucinations are incorrect outputs, not accurate ones."
            },
            {
                answer: "Incorrect or nonsensical outputs generated by an AI", isCorrect: true, feedback: "Hallucinations are instances where the AI generates incorrect or nonsensical information, often due to ambiguity in prompts or gaps in training data."
            },
            {
                answer: "Consistent outputs due to a large training dataset", isCorrect: false, feedback: "Hallucinations are inconsistent and often unpredictable, rather than consistent."
            },
        ]
    },
    {
        question: "Which strategy helps reduce hallucinations in Generative AI models?",
        answers: [
            {
                answer: "Simple and vague prompting", isCorrect: false, feedback: "Simple and vague prompts can increase the likelihood of hallucinations since the AI has little context to guide its responses accurately."
            },
            {
                answer: "Tool integration", isCorrect: true, feedback: "Tool integration allows AI to interact with external tools, like calculators or accounting software, reducing errors by relying on specialized tools for specific tasks, thereby minimizing hallucinations."
            },
            {
                answer: "Avoiding human review of outputs", isCorrect: false, feedback: "Human review is essential in reducing the impact of hallucinations, as the AI’s output needs careful scrutiny for accuracy."
            },
            {
                answer: "Restricting model training to a smaller dataset", isCorrect: false, feedback: "Restricting model training to a smaller dataset typically decreases model capabilities, potentially leading to more errors rather than fewer."
            },
        ]
    },
    {
        question: "Which of the following is a risk associated with tool integration in GenAI models?",
        answers: [
            {
                answer: "Increased likelihood of AI hallucinations", isCorrect: false, feedback: "Tool integration generally reduces hallucinations by allowing the model to use specialized tools for accuracy."
            },
            {
                answer: "Enhanced model creativity", isCorrect: false, feedback: "Tool integration improves functionality and accuracy but does not directly enhance creativity."
            },
            {
                answer: "Security vulnerabilities due to external tool access", isCorrect: true, feedback: "Allowing AI to access external tools and databases introduces risks, such as data privacy issues and potential unauthorized access, which are significant security concerns."
            },
            {
                answer: "Elimination of the need for human oversight", isCorrect: false, feedback: "Tool integration doesn’t eliminate the need for human oversight; humans are still necessary to ensure the data is used appropriately and safely."
            },
        ]
    },
    {
        question: "How can prompting be improved to minimize hallucinations in GenAI?",
        answers: [
            {
                answer: "Using vague instructions to encourage creativity", isCorrect: false, feedback: "Vague instructions can lead to inconsistent outputs and increase hallucinations, as the AI lacks direction."
            },
            {
                answer: "Specifying detailed context and constraints", isCorrect: true, feedback: "Providing detailed context, specifying boundaries, and requesting step-by-step reasoning help guide the model and reduce the risk of hallucinations."
            },
            {
                answer: "Limiting prompts to yes/no questions", isCorrect: false, feedback: "Limiting prompts to yes/no questions restricts the model’s ability to provide valuable insights and doesn’t address the complexity needed to avoid hallucinations."
            },
            {
                answer: `Asking the model to self-correct with "do not hallucinate"`, isCorrect: false, feedback: `Asking the model to "not hallucinate" is not effective, as the AI does not inherently understand this as a control mechanism.`
            },
        ]
    },
]